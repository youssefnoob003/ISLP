{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbad046",
   "metadata": {},
   "source": [
    "**1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.**\n",
    "\n",
    "We are given the logistic function:\n",
    "\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "1 - p(X) = 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \n",
    "$$\n",
    "$$\n",
    "= \\frac{1 + e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\frac{p(X)}{1 - p(X)} = \\frac{\\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}}{\\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}} = e^{\\beta_0 + \\beta_1 X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05aa6e",
   "metadata": {},
   "source": [
    "**2. It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the $k^{th}$ class are drawn from a $N(\\mu_k, \\sigma^2)$ distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.**\n",
    "\n",
    "We classify an observation $x$ to the class that maximizes the posterior probability:\n",
    "\n",
    "$$\n",
    "p_k(x) = \\frac{\\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{1}{2\\sigma^2}(x - \\mu_k)^2 \\right)}{\\sum_{l=1}^K \\pi_l \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{1}{2\\sigma^2}(x - \\mu_l)^2 \\right)}\n",
    "$$\n",
    "\n",
    "Since the denominator is the same for all $k$, maximizing $p_k(x)$ is equivalent to maximizing the **numerator**:\n",
    "\n",
    "$$\n",
    "\\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{1}{2\\sigma^2}(x - \\mu_k)^2 \\right)\n",
    "$$\n",
    "\n",
    "Taking the log (a strictly increasing function, so the argmax doesn't change), we get:\n",
    "\n",
    "$$\n",
    "log(\\pi_k) - log(\\sqrt{2\\pi}\\sigma) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2\n",
    "$$\n",
    "\n",
    "Since $log(\\sqrt{2\\pi}\\sigma)$ is constant across all classes, it can be dropped. So, the class that **maximizes**:\n",
    "\n",
    "$$\n",
    "log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2\n",
    "$$\n",
    "\n",
    "is the class to which we assign $x$. Expanding the quadratic:\n",
    "\n",
    "$$\n",
    "(x - \\mu_k)^2 = x^2 - 2x\\mu_k + \\mu_k^2\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "log(\\pi_k) - \\frac{1}{2\\sigma^2}(x^2 - 2x\\mu_k + \\mu_k^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k)\n",
    "$$\n",
    "\n",
    "Since $x^2$ is constant with respect to $k$, we can drop it when determining which $\\delta_k(x)$ is largest. So we define the simplified discriminant function as:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c0b80",
   "metadata": {},
   "source": [
    "**3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the $k^{th}$ class then X comes from a one-dimensional normal distribution, $X \\sim N(\\mu_k, \\sigma_k^2)$. Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.**\n",
    "\n",
    "Assume $X \\sim N(\\mu_k, \\sigma_k^2)$, i.e., each class has its own mean and variance. Then the class-conditional density is:\n",
    "\n",
    "$$\n",
    "f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left( -\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2 \\right)\n",
    "$$\n",
    "\n",
    "According to the Bayes classifier, we assign $x$ to the class that maximizes the posterior probability:\n",
    "\n",
    "$$\n",
    "p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n",
    "$$\n",
    "\n",
    "Since the denominator is common across all classes, we only need to maximize the numerator $\\pi_k f_k(x)$:\n",
    "\n",
    "$$\n",
    "\\pi_k f_k(x) = \\pi_k \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left( -\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2 \\right)\n",
    "$$\n",
    "\n",
    "Taking the logarithm of this expression:\n",
    "\n",
    "$$\n",
    "log(\\pi_k f_k(x)) = log(\\pi_k) - log(\\sqrt{2\\pi}) - log(\\sigma_k) - \\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2\n",
    "$$\n",
    "\n",
    "Dropping the constant $log(\\sqrt{2\\pi})$, which is the same for all classes, we have:\n",
    "\n",
    "$$\n",
    "log\\left( \\frac{\\pi_k}{\\sigma_k} \\right) - \\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2\n",
    "$$\n",
    "\n",
    "Expanding the quadratic term:\n",
    "\n",
    "$$\n",
    "= log\\left( \\frac{\\pi_k}{\\sigma_k} \\right) - \\frac{x^2}{2\\sigma_k^2} + \\frac{x \\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}\n",
    "$$\n",
    "\n",
    "The discriminant function clearly contains a quadratic term in $x$ (namely $-\\frac{x^2}{2\\sigma_k^2}$), so the decision boundary is not linear. **Therefore, under the QDA model with class-specific variances, the Bayes classifier is quadratic in $x$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca366ee9",
   "metadata": {},
   "source": [
    "**4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.**\n",
    "\n",
    "**(a) Suppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?**\n",
    "\n",
    "In this problem, there are two plausible interpretations due to how the KNN window behaves near the boundaries of the interval $[0, 1]$. Specifically, when $X < 0.05$ or $X > 0.95$, the symmetric 10% window would extend beyond the range of the data, requiring a special treatment.\n",
    "\n",
    "**Approach 1: Truncating the Window at the Boundaries**\n",
    "\n",
    "In this approach, we truncate the window when it reaches the boundaries. For example, if $x = 0$, we consider only observations in the range $[0, 0.05]$, rather than attempting to extend to negative values. The effective window width therefore depends on the position of $x$.\n",
    "\n",
    "We break the domain $[0, 1]$ into three regions:\n",
    "\n",
    "* **Interior region**: For $x \\in [0.05, 0.95]$, we use the full symmetric window $[x - 0.05, x + 0.05]$, resulting in a constant window width of 0.1. Since this applies to 90% of the interval, it contributes:\n",
    "\n",
    "  $$\n",
    "  0.9 \\times 0.1 = 0.09\n",
    "  $$\n",
    "\n",
    "* **Left boundary region**: For $x \\in [0, 0.05]$, we truncate the window to $[0, x + 0.05]$, whose width is $x + 0.05$. The average width over this interval is:\n",
    "\n",
    "  $$\n",
    "  \\frac{1}{0.05} \\int_0^{0.05} (x + 0.05)\\,dx = \\frac{1}{0.05} \\left[ \\frac{x^2}{2} + 0.05x \\right]_0^{0.05} = 0.075\n",
    "  $$\n",
    "\n",
    "  So this region contributes:\n",
    "\n",
    "  $$\n",
    "  0.05 \\times 0.075 = 0.00375\n",
    "  $$\n",
    "\n",
    "* **Right boundary region**: Similarly, for $x \\in [0.95, 1]$, we truncate the window to $[x - 0.05, 1]$, which also has an average width of 0.075. So it also contributes:\n",
    "\n",
    "  $$\n",
    "  0.05 \\times 0.075 = 0.00375\n",
    "  $$\n",
    "\n",
    "Adding all contributions:\n",
    "\n",
    "$$\n",
    "0.00375 + 0.09 + 0.00375 = 0.0975\n",
    "$$\n",
    "\n",
    "So, under the truncation approach, the average fraction of observations used to make a prediction is **9.75%**.\n",
    "\n",
    "**Approach 2: Shifting the Window at the Boundaries**\n",
    "\n",
    "In this approach, we maintain a constant window width of 0.1 (10% of the range), regardless of the location of $x$. However, at the boundaries of the domain $[0, 1]$, the window is no longer centered around $x$. Instead, we shift the window to stay within the domain.\n",
    "\n",
    "* For $x < 0.05$, we fix the window as $[0, 0.1]$, so all test points near the left edge use the same set of neighbors.\n",
    "* For $x > 0.95$, we fix the window as $[0.9, 1]$, similarly assigning the same neighbors for all test points near the right edge.\n",
    "\n",
    "This ensures a consistent number of observations is used (i.e., 10% of the dataset) for all predictions. However, this comes at a cost: for all test points within those extreme intervals, the predicted response will be the same, since they rely on an identical subset of training observations. This reduces local adaptiveness near the boundaries and can lead to biased or overly smooth predictions in those regions.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "In choosing between the two approaches, the trade-off lies between consistency and adaptability. **Approach 1** (truncating the window) uses a smaller number of observations near the boundaries, which slightly reduces the average amount of data used (9.75% instead of 10%), but maintains local adaptiveness, allowing predictions to vary even at the edges. **Approach 2** (shifting the window) ensures a fixed number of observations for every prediction, but sacrifices flexibility near the boundaries, as all test points in those edge regions share the same neighbors. In practice, truncation (Approach 1) is often preferred when preserving local structure and minimizing bias near boundaries is more important than strictly using the same number of points for every estimate.\n",
    "\n",
    "**(b) Now suppose that we have a set of observations, each with measurements on $p = 2$ features, $X_1$ and $X_2$. We assume that $(X_1, X_2)$ are uniformly distributed on $[0, 1]\\times[0, 1]$. We wish to predict a test observation’s response using only observations that are within 10% of the range of $X_1$ and within 10% of the range of $X_2$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in the range [0.55, 0.65] for $X_1$ and in the range [0.3, 0.4] for $X_2$. On average, what fraction of the available observations will we use to make the prediction?**\n",
    "\n",
    "For the remainder of the exercise, we adopt the **window truncation** approach. In the one-dimensional case, we previously showed that the average fraction of observations used is **0.0975**. Since the features $X_1$ and $X_2$ are independently and uniformly distributed on $[0, 1]$, the prediction window in two dimensions becomes the product of two such intervals. Therefore, the average fraction of observations used in the 2D case is:\n",
    "$$\n",
    "0.0975 \\times 0.0975 = 0.00950625\n",
    "$$\n",
    "That is, on average, approximately **0.95%** of the total observations are used for each prediction.\n",
    "\n",
    "**(c) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?**\n",
    "\n",
    "As in part (b), for each individual feature, we include observations within the closest 10% of the feature's range. From part (a), we saw that using the window truncation approach in 1D results in an average coverage of approximately **0.0975**.\n",
    "\n",
    "Since features are independent and the window must fall within 10% of the range **for all 100 features simultaneously**, the overall fraction of the data used is: $(0.0975)^{100} \\approx 0$\n",
    "\n",
    "**(d) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.**\n",
    "\n",
    "In part (a), using 10% of the range of $X$ to define the neighborhood seemed effective: on average, we used about 9.75% of the available observations for prediction. However, in part (b), when we moved from one to two features, the average fraction of observations used dropped to approximately 0.95%—a tenfold decrease—highlighting how quickly the neighborhood becomes sparse even with a modest increase in dimensionality. In part (c), increasing the number of features to 100 caused the fraction of observations used to drop essentially to zero. This illustrates a key drawback of KNN in high dimensions: the **curse of dimensionality**. As the number of predictors $p$ increases, the volume of the space grows exponentially, and the proportion of nearby observations rapidly diminishes. Consequently, KNN becomes ineffective because it relies on the assumption that test points will have sufficiently close neighbors in the training data, an assumption that breaks down as dimensionality increases.\n",
    "\n",
    "**(e) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer.**\n",
    "\n",
    "To ensure the hypercube contains, on average, 10% of the training observations, its **volume** must be 0.1. In a $p$-dimensional space, the volume of a hypercube is the side length raised to the power $p$, so:\n",
    "\n",
    "$$\n",
    "l^p = 0.1 \\quad \\Rightarrow \\quad l = 0.1^{1/p}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "* For **$p = 1$**, the side length is:\n",
    "\n",
    "  $$\n",
    "  l = 0.1^{1/1} = 0.1\n",
    "  $$\n",
    "* For **$p = 2$**, the side length is:\n",
    "\n",
    "  $$\n",
    "  l = 0.1^{1/2} \\approx 0.316\n",
    "  $$\n",
    "* For **$p = 100$**, the side length is:\n",
    "\n",
    "  $$\n",
    "  l = 0.1^{1/100} \\approx 0.977\n",
    "  $$\n",
    "\n",
    "As the number of dimensions increases, the side length must grow significantly in order to maintain the same volume. For $p = 100$, the side length approaches the full feature range $[0, 1]$, meaning the hypercube spans nearly the entire space. This highlights a core issue in high-dimensional settings: to include enough nearby points, we must reach far from the test observation in every direction. This makes **KNN models very inflexible in high dimensions**, as most predictions effectively become averages over nearly the whole dataset—resulting in overly smooth, non-local behavior. This is yet another concrete example of the **curse of dimensionality**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06baed03",
   "metadata": {},
   "source": [
    "**5. We now examine the differences between LDA and QDA.**\n",
    "\n",
    "**(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?**\n",
    "\n",
    "Since QDA is a more flexible model, it can better adapt to the training data, even fitting noise. As a result, it will generally achieve lower training error than LDA, regardless of the true underlying decision boundary. However, because the Bayes decision boundary is actually linear in this case, LDA is correctly specified and will better generalize to new, unseen data. Therefore, although QDA may perform better on the training set, LDA is expected to yield better test performance due to its lower variance and more accurate assumptions.\n",
    "\n",
    "**(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?**\n",
    "\n",
    "When the true decision boundary is non-linear, QDA is better suited to model the underlying relationship because it allows for class-specific covariance matrices, enabling non-linear boundaries. On the training set, QDA will again perform better due to its flexibility. More importantly, on the test set, QDA will also likely outperform LDA, since LDA imposes an incorrect linearity assumption, resulting in higher bias. In this setting, QDA’s flexibility becomes an asset rather than a liability.\n",
    "\n",
    "**(c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?**\n",
    "\n",
    "As the sample size increases, QDA's test prediction accuracy is expected to improve relative to LDA. This is because QDA estimates more parameters (specifically, a full covariance matrix per class), and those estimates become more reliable with more data. The larger the sample size, the more the variance of these estimates decreases, reducing the risk of overfitting. Therefore, with sufficient data, the flexibility of QDA becomes less of a disadvantage and can lead to better test performance, particularly when the true boundary is non-linear. While QDA can occasionally outperform LDA on test data in a linear problem (especially with large sample sizes), this would be due to randomness or minor structure in the data — not because QDA is the better theoretical choice. In expectation, LDA still wins when the true boundary is linear.\n",
    "\n",
    "**(d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.**\n",
    "\n",
    "False. While QDA is indeed flexible enough to model a linear boundary, its flexibility is unnecessary when the true decision boundary is already linear. This extra flexibility introduces additional variance into the model, making it more prone to overfitting. In contrast, LDA makes the correct assumption and benefits from lower variance, resulting in a lower test error rate in this case. Therefore, using QDA in this setting is not advantageous and may actually worsen generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ed275",
   "metadata": {},
   "source": [
    "**6. Suppose we collect data for a group of students in a statistics class with variables $X_1$ = hours studied, $X_2$ = undergrad GPA, and $Y$ = receive an A. We fit a logistic regression and produce estimated coefficient, $\\hat{\\beta_0} = −6, \\hat{\\beta_1} = 0.05, \\hat{\\beta_2} = 1$.**\n",
    "\n",
    "**(a) Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.**\n",
    "\n",
    "To estimate the probability that a student who studies for 40 hours and has an undergraduate GPA of 3.5 receives an A, we plug the values into the logistic regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_0} + \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2 = -6 + 0.05 \\times 40 + 1 \\times 3.5 = -0.5\n",
    "$$\n",
    "\n",
    "Then, applying the logistic function:\n",
    "\n",
    "$$\n",
    "P(Y = 1) = \\frac{e^{-0.5}}{1 + e^{-0.5}} \\approx 0.3775\n",
    "$$\n",
    "\n",
    "So, the estimated probability of getting an A is approximately **37.75%**.\n",
    "\n",
    "**(b) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?**\n",
    "\n",
    "To find how many hours the student must study to have a **50% chance** of receiving an A (i.e., $P(Y = 1) = 0.5$), we solve the equation:\n",
    "\n",
    "$$\n",
    "log\\left(\\frac{0.5}{1 - 0.5}\\right) = \\hat{\\beta_0} + \\hat{\\beta_1} X_1 + \\hat{\\beta_2} X_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 = -6 + 0.05 X_1 + 3.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "0.05 X_1 = 2.5 \\Rightarrow X_1 = 50\n",
    "$$\n",
    "\n",
    "Therefore, the student would need to study **50 hours** to have a 50% chance of receiving an A in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd9ec5",
   "metadata": {},
   "source": [
    "**7. Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on X, last year’s percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was $\\bar{X} = 10$, while the mean for those that didn’t was $\\bar{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $\\hat{\\sigma}^2 = 36$. Finally, 80% of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was $X = 4$ last year.**\n",
    "\n",
    "We aim to predict whether a company will issue a dividend this year (“Yes” or “No”) based on last year’s percentage profit, denoted by $X$:\n",
    "\n",
    "* Companies that issued a dividend had a mean profit of $\\mu_1 = 10$,\n",
    "* Companies that did not issue a dividend had a mean profit of $\\mu_2 = 0$,\n",
    "* The variance of $X$ is $\\sigma^2 = 36$ for both groups,\n",
    "* 80% of companies issued a dividend, i.e., $\\pi_1 = 0.8$, $\\pi_2 = 0.2$.\n",
    "\n",
    "Assuming that $X$ is normally distributed within each group, we want to compute the probability that a company will issue a dividend given it had a 4% profit last year, i.e., $X = 4$.\n",
    "\n",
    "For the dividend-issuing group (class 1):\n",
    "\n",
    "$$\n",
    "f_1(4) = \\frac{1}{\\sqrt{2\\pi \\cdot 36}} \\exp\\left(-\\frac{(4 - 10)^2}{2 \\cdot 36}\\right) = \\frac{1}{\\sqrt{72\\pi}} \\cdot e^{-0.5} \\approx 0.0403\n",
    "$$\n",
    "\n",
    "For the non-dividend group (class 2):\n",
    "\n",
    "$$\n",
    "f_2(4) = \\frac{1}{\\sqrt{2\\pi \\cdot 36}} \\exp\\left(-\\frac{(4 - 0)^2}{2 \\cdot 36}\\right) = \\frac{1}{\\sqrt{72\\pi}} \\cdot e^{-0.2222} \\approx 0.0532\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Dividend} \\mid X = 4) = \\frac{\\pi_1 f_1(4)}{\\pi_1 f_1(4) + \\pi_2 f_2(4)} = \\frac{0.8 \\cdot 0.0403}{0.8 \\cdot 0.0403 + 0.2 \\cdot 0.0532}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.03224}{0.03224 + 0.01064} = \\frac{0.03224}{0.04288} \\approx 0.7515\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe6998",
   "metadata": {},
   "source": [
    "**8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. $K = 1$) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?**\n",
    "\n",
    "We should **prefer logistic regression** for classifying new observations, **despite** the lower *average* error reported by 1-nearest neighbors (1-NN). The key issue is how error is distributed between training and test sets.\n",
    "\n",
    "The logistic regression model has a **20% training error** and a **30% test error**, indicating a moderate gap between the two. This suggests it's generalizing reasonably well, though it's slightly underfitting.\n",
    "\n",
    "In contrast, 1-NN will always have **0% training error**, because each training point is classified using itself. So, if the average error rate across training and test data is **18%**, and the training error is 0%, then the test error must be **36%** (since the average of 0% and 36% is 18%).\n",
    "\n",
    "Thus, while 1-NN performs perfectly on training data, it generalizes worse than logistic regression, with a **higher test error** (36% vs. 30%). Since the goal is to perform well on **unseen data**, we should choose **logistic regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a6a64",
   "metadata": {},
   "source": [
    "**9. This problem has to do with odds.**\n",
    "\n",
    "**(a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?**\n",
    "\n",
    "Recall that the relationship between probability and odds is given by:\n",
    "\n",
    "$$\n",
    "\\text{odds} = \\frac{p(x)}{1 - p(x)} \\Rightarrow p(x) = \\frac{\\text{odds}}{1 + \\text{odds}}\n",
    "$$\n",
    "\n",
    "Substituting in the given odds:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{0.37}{1 + 0.37} = \\frac{0.37}{1.37} \\approx 0.27\n",
    "$$\n",
    "\n",
    "So, approximately **27%** of people with odds of 0.37 will default.\n",
    "\n",
    "**(b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?**\n",
    "\n",
    "Using the same relationship:\n",
    "\n",
    "$$\n",
    "\\text{odds} = \\frac{p(x)}{1 - p(x)} = \\frac{0.16}{1 - 0.16} = \\frac{0.16}{0.84} \\approx 0.19\n",
    "$$\n",
    "\n",
    "So, the odds of defaulting are approximately **0.19**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acab92",
   "metadata": {},
   "source": [
    "**10. Equation 4.32 derived an expression for $log(\\frac{Pr(Y =k|X=x)}{Pr(Y =K|X=x)})$ in the setting where $p > 1$, so that the mean for the $k^{th}$ class, $\\mu_k$, is a p-dimensional vector, and the shared covariance $\\sum$ is a $p \\times p$ matrix. However, in the setting with $p = 1$, (4.32) takes a simpler form, since the means $\\mu_1$, ..., $\\mu_K$ and the variance $\\sigma^2$ are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for $a_k$ and $b_{kj}$ in terms of $\\pi_k$, $\\pi_K$, $\\mu_k$, $\\mu_K$, and $\\sigma^2$**\n",
    "\n",
    "We know that:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\log\\left(\\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)}\\right) = \\log\\left(\\frac{\\pi_k \\exp\\left(-\\frac{1}{2}(x - \\mu_k)^\\top \\Sigma^{-1}(x - \\mu_k)\\right)}{\\pi_K \\exp\\left(-\\frac{1}{2}(x - \\mu_K)^\\top \\Sigma^{-1}(x - \\mu_K)\\right)}\\right)\n",
    "$$\n",
    "\n",
    "Since $p = 1$, $\\Sigma$ is a scalar equal to $\\sigma^2$, so $\\Sigma^{-1} = \\frac{1}{\\sigma^2}$.\n",
    "Also, $(x - \\mu)^\\top$ is scalar, so $(x - \\mu)^\\top (x - \\mu) = (x - \\mu)^2$.\n",
    "\n",
    "$$\n",
    "\\Longrightarrow \\log\\left(\\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)}{\\pi_K \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_K)^2\\right)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2\\sigma^2} \\left[(x - \\mu_k)^2 - (x - \\mu_K)^2\\right]\n",
    "$$\n",
    "\n",
    "Expanding the squares:\n",
    "\n",
    "$$\n",
    "= \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2\\sigma^2} \\left[\\mu_k^2 - \\mu_K^2 - 2x(\\mu_k - \\mu_K)\\right]\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "= \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) + \\frac{1}{2\\sigma^2}(\\mu_K^2 - \\mu_k^2) + \\frac{x}{\\sigma^2}(\\mu_k - \\mu_K)\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "= a_k + b_k x\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "a_k = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) + \\frac{1}{2\\sigma^2}(\\mu_K^2 - \\mu_k^2) \\quad \\text{and} \\quad b_k = \\frac{\\mu_k - \\mu_K}{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c5b79",
   "metadata": {},
   "source": [
    "**11. Work out the detailed forms of a $k$, $b_{kj}$, and $c_{kjl}$ in (4.33). Your answer should involve $\\pi_k$, $\\pi_K$, $\\mu_k$, $\\mu_K$, $\\Sigma_k$, and $\\Sigma_K$**\n",
    "\n",
    "For QDA, the class-conditional densities are multivariate Gaussians with class-specific means $\\mu_k$​ and class-specific covariance matrices $\\Sigma_k$​. The density for class $k$ is:\n",
    "$$\n",
    "f_k(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma_k|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^\\top\\Sigma^{-1}_k(x-\\mu_k))\n",
    "$$\n",
    "$$\n",
    "\\Longrightarrow \\log\\left(\\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)}\\right)\n",
    "$$\n",
    "$$\n",
    "= \\log\\left(\\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)}\\right) = \\log\\left(\\frac{\\pi_k |\\Sigma_K|^{1/2} \\exp\\left(-\\frac{1}{2}(x - \\mu_k)^\\top \\Sigma_k^{-1}(x - \\mu_k)\\right)}{\\pi_K |\\Sigma_k|^{1/2} \\exp\\left(-\\frac{1}{2}(x - \\mu_K)^\\top \\Sigma_K^{-1}(x - \\mu_K)\\right)}\\right)\n",
    "$$\n",
    "$$\n",
    "= \\log(\\frac{\\pi_k}{\\pi_K}) - \\frac{1}{2} \\log(\\frac{|\\Sigma_k|}{|\\Sigma_K|}) - \\frac{1}{2}(x - \\mu_k)^\\top \\Sigma_k^{-1}(x - \\mu_k) + \\frac{1}{2}(x - \\mu_K)^\\top \\Sigma_K^{-1}(x - \\mu_K)\n",
    "$$\n",
    "$$\n",
    "= \\log(\\frac{\\pi_k}{\\pi_K}) - \\frac{1}{2} \\log(\\frac{|\\Sigma_k|}{|\\Sigma_K|}) - \\frac{1}{2}(x^\\top\\Sigma_k^{-1}x -2x\\Sigma_k^{-1}\\mu_k^\\top + \\mu_k^\\top\\Sigma_k^{-1}\\mu_k) + \\frac{1}{2}(x^\\top\\Sigma_K^{-1}x -2x\\Sigma_K^{-1}\\mu_K^\\top + \\mu_K^\\top\\Sigma_K^{-1}\\mu_K)\n",
    "$$\n",
    "$$\n",
    "= \\log(\\frac{\\pi_k}{\\pi_K}) - \\frac{1}{2} \\log(\\frac{|\\Sigma_k|}{|\\Sigma_K|}) - \\frac{1}{2}(\\mu_k^\\top\\Sigma_k^{-1}\\mu_k - \\mu_K^\\top\\Sigma_K^{-1}\\mu_K) + (x\\Sigma_k^{-1}\\mu_k^\\top - x^\\top\\Sigma_K^{-1}\\mu_K^\\top) - \\frac{1}{2}(x^\\top\\Sigma_k^{-1}x - x^\\top\\Sigma_K^{-1}x)\n",
    "$$\n",
    "$$\n",
    "= \\log(\\frac{\\pi_k}{\\pi_K}) - \\frac{1}{2} \\log(\\frac{|\\Sigma_k|}{|\\Sigma_K|}) - \\frac{1}{2}(\\mu_k^\\top\\Sigma_k^{-1}\\mu_k - \\mu_K^\\top\\Sigma_K^{-1}\\mu_K) + \\sum_j^p x_j (\\Sigma_k^{-1}\\mu_k - \\Sigma_K^{-1}\\mu_K)_j - \\frac{1}{2} (\\sum_j^p\\sum_j^p x_j x_l \\Sigma_k^{-1} - \\sum_j^p\\sum_j^p x_j x_l \\Sigma_K^{-1})\n",
    "$$\n",
    "$$\n",
    "= a_k + \\sum^p_j b_{kj}x_j + \\sum_j^p\\sum_l^p c_{kjl}x_jx_l\n",
    "$$\n",
    "With:\n",
    "$$\n",
    "a_k = \\log(\\frac{\\pi_k}{\\pi_K}) - \\frac{1}{2} \\log(\\frac{|\\Sigma_k|}{|\\Sigma_K|}) - \\frac{1}{2}(\\mu_k^\\top\\Sigma_k^{-1}\\mu_k - \\mu_K^\\top\\Sigma_K^{-1}\\mu_K)\n",
    "$$\n",
    "$$\n",
    "b_{kj} = (\\Sigma_k^{-1}\\mu_k - \\Sigma_K^{-1}\\mu_K)_j\n",
    "$$\n",
    "$$\n",
    "c_{kjl} = (\\Sigma_k^{-1} - \\Sigma_K^{-1})_{jl}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe3684",
   "metadata": {},
   "source": [
    "**12. Suppose that you wish to classify an observation $X \\in \\R$ into apples and oranges. You fit a logistic regression model and find that**\n",
    "\n",
    "$$\n",
    "\\hat{\\Pr}(Y = \\text{orange} \\mid X = x) = \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 x)}{1 + \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 x)}\n",
    "$$\n",
    "\n",
    "**Your friend fits a logistic regression model using the softmax formulation (4.13):**\n",
    "\n",
    "$$\n",
    "\\hat{\\Pr}(Y = \\text{orange} \\mid X = x) = \\frac{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x)}{\\exp(\\hat{\\alpha}_{\\text{orange},0} + \\hat{\\alpha}_{\\text{orange},1} x) + \\exp(\\hat{\\alpha}_{\\text{apple},0} + \\hat{\\alpha}_{\\text{apple},1} x)}\n",
    "$$\n",
    "\n",
    "**(a) What is the log-odds of orange versus apple in your model?**\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{\\hat{\\Pr}(Y = \\text{orange} \\mid X = x)}{\\hat{\\Pr}(Y = \\text{apple} \\mid X = x)}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "$$\n",
    "\n",
    "**(b) What is the log-odds of orange versus apple in your friend’s model?**\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{\\hat{\\Pr}(Y = \\text{orange} \\mid X = x)}{\\hat{\\Pr}(Y = \\text{apple} \\mid X = x)}\\right) = (\\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0}) + (\\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1})x\n",
    "$$\n",
    "\n",
    "**(c) Suppose in your model, $\\hat{\\beta}_0 = 2$ and $\\hat{\\beta}_1 = -1$. What are the coefficient estimates in your friend’s model?**\n",
    "\n",
    "We know that:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 = \\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0} \\quad \\text{and} \\quad \\hat{\\beta}_1 = \\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Any values that satisfy\n",
    "  $\\hat{\\alpha}_{\\text{orange},0} - \\hat{\\alpha}_{\\text{apple},0} = 2$\n",
    "  $\\hat{\\alpha}_{\\text{orange},1} - \\hat{\\alpha}_{\\text{apple},1} = -1$\n",
    "  are valid.\n",
    "\n",
    "**(d) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates $\\hat{\\alpha}_{\\text{orange},0} = 1.2$, $\\hat{\\alpha}_{\\text{orange},1} = -2$, $\\hat{\\alpha}_{\\text{apple},0} = 3$, $\\hat{\\alpha}_{\\text{apple},1} = 0.6$. What are the coefficient estimates in your model?**\n",
    "\n",
    "Your model must satisfy:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0 = 1.2 - 3 = -1.8 \\quad \\text{and} \\quad \\hat{\\beta}_1 = -2 - 0.6 = -2.6\n",
    "$$\n",
    "\n",
    "**(e) Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.**\n",
    "\n",
    "100%, Both models produce the **same probabilities** for the classes, because the softmax with two classes is mathematically equivalent to the sigmoid (logistic) model. The representations differ, but the **decision boundary is identical**, so the predicted class labels will agree **for all observations**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
