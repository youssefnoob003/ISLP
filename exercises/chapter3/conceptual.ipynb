{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62585dc3",
   "metadata": {},
   "source": [
    "**1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio , and newspaper , rather than in terms of the coefficients of the linear model.**\n",
    "\n",
    "|             | Coefficient | Std. Error | t-Statistic | p-Value  |\n",
    "|-------------|-------------|------------|-------------|----------|\n",
    "| Intercept   | 2.939       | 0.3119     | 9.42        | < 0.0001 |\n",
    "| TV          | 0.046       | 0.0014     | 32.81       | < 0.0001 |\n",
    "| Radio       | 0.189       | 0.0086     | 21.89       | < 0.0001 |\n",
    "| Newspaper   | -0.001      | 0.0059     | -0.18       | 0.8599   |\n",
    "\n",
    "\n",
    "The p-values in the table correspond to the null hypotheses that each type of advertising, TV, radio, and newspaper, has no effect on sales. In other words, they test whether each predictor is statistically significantly associated with sales. Based on these results, we conclude that TV and Radio do affect sales. In contrast, the large p-value for newspaper advertising (0.8599) indicates that we fail to reject the null hypothesis for this variable, implying that newspaper advertising does not have a statistically significant association with sales in the presence of the other predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5c44b",
   "metadata": {},
   "source": [
    "**2. Carefully explain the differences between the KNN classifier and KNN regression methods.**\n",
    "\n",
    "While both KNN classification and KNN regression are based on the same core idea, the way they generate predictions is different. In KNN classification, the method assigns a class label to a new data point based on a majority vote among its K nearest neighbors. That is, it predicts the class that is most frequent among the neighboring observations. KNN regression predicts a numerical value by taking the average of the response values of the K nearest neighbors. So, while KNN classification outputs a category, KNN regression outputs a continuous value, even though both rely on proximity in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c9bf9",
   "metadata": {},
   "source": [
    "**3. Suppose we have a data set with five predictors, $X_1=$ GPA, $X_2=$ IQ, $X_3=$ Level (1 for College and 0 for High School), $X_4=$ Interaction between GPA and IQ, and $X_5=$ Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\\hat{\\beta}_0=50$, $\\hat{\\beta}_1=20$, $\\hat{\\beta}_2=0.07$, $\\hat{\\beta}_3=35$, $\\hat{\\beta}_4=0.01$, $\\hat{\\beta}_5=−10$.**\n",
    "\n",
    "**(a) Which answer is correct, and why?** For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "\n",
    "$\\text{Salary} = \\hat{\\beta}_0 + \\hat{\\beta}_1 * \\text{GPA} + \\hat{\\beta}_2 * \\text{IQ} + \\hat{\\beta}_3 * \\text{Level} + \\hat{\\beta}_4 * \\text{GPA} * \\text{IQ} + \\hat{\\beta}_5 * \\text{GPA} * \\text{Level}$\n",
    "\n",
    "Ignoring the fixed terms, GPA and IQ we get:\n",
    "\n",
    "$\\text{Salary} = 35 * \\text{Level} - 10 * \\text{GPA} * \\text{Level} + \\text{constant}$\n",
    "\n",
    "For college graduates: $\\text{Salary} = 35 - 10 * \\text{GPA} + \\text{constant}$ and for highschool graduates: $\\text{Salary} = \\text{constant}$\n",
    "\n",
    "The difference between the two salaries is $\\text{Salary\\_college} - \\text{Salary\\_highschool} = 35 - 10 * \\text{GPA}$.\n",
    "\n",
    "So for $\\text{GPA} < 3.5$, collge graduates would earn more.\n",
    "\n",
    "**(b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.**\n",
    "\n",
    "$$ \n",
    "\\text{Salary} = \\hat{\\beta}_0 + \\hat{\\beta}_1 * \\text{GPA} + \\hat{\\beta}_2 * \\text{IQ} + \\hat{\\beta}_3 * \\text{Level} + \\hat{\\beta}_4 * \\text{GPA} * \\text{IQ} + \\hat{\\beta}_5 * \\text{GPA} * \\text{Level}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 50 + 20 * 4 + 0.07 * 110 + 35 * 1 + 0.01 * 110 * 4 - 10 * 4 * 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 137.1\n",
    "$$\n",
    "\n",
    "**(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.**\n",
    "\n",
    "**False.** The magnitude of the coefficient (0.01) alone does not determine whether there is evidence of an interaction effect. What matters is not just the size of the coefficient, but also whether it is **statistically significant**. A small coefficient can still be significant if the associated p-value is small and the standard error is low. Additionally, even if the numerical effect seems small (e.g., GPA = 4.0 and IQ = 150 leads to an added value of $0.01 \\times 4 \\times 150 = 6$ thousand dollars), this could still be meaningful depending on the context, especially if salary ranges are tight. Without knowing the p-value or confidence interval for the interaction term, we cannot conclude whether there is strong or weak evidence for the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9727c98",
   "metadata": {},
   "source": [
    "**4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y=\\beta_0 + \\beta_1 * X + \\beta_2 * X^2 + \\beta_3 * X^3 + \\epsilon$**\n",
    "\n",
    "**(a) Suppose that the true relationship between X and Y is linear, i.e. $Y = \\beta_0 + \\beta_1 * X + \\epsilon$.  Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.**\n",
    "\n",
    "Since the true relationship between X and Y is linear, the linear regression model correctly specifies the underlying relationship. However, the cubic regression model is more flexible because it includes additional polynomial terms. In general, regardless of the true form of the relationship, more flexible models tend to achieve a lower training RSS because they have more capacity to fit not only the signal but also the noise in the data, resulting in predictions that more closely follow the observed values.\n",
    "\n",
    "**(b) Answer (a) using test rather than training RSS.**\n",
    "\n",
    "The linear regression model will result in a lower test RSS since it correctly captures the true relationship between X and Y. The cubic model, while more flexible, overfits the training observations by fitting not just the signal but also the noise. Therefore, on new unseen data, the linear model should generalize better and yield a lower test RSS compared to the overly complex cubic model.\n",
    "\n",
    "**(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.**\n",
    "\n",
    "There is no uncertainty about the training RSS: the cubic regression model will always have training RSS that is equal to or lower than that of the linear regression model. This is because the cubic model includes all the terms in the linear model, along with additional polynomial terms that allow it to better fit the data. Even if the true relationship is not highly non-linear, the cubic model can still use its extra flexibility to reduce residual error on the training set.\n",
    "\n",
    "**(d) Answer (c) using test rather than training RSS.**\n",
    "\n",
    "There isn't enough information to determine which model will have the lower test RSS because we don't know how far the true relationship deviates from linearity. If the true relationship is only mildly non-linear, say, involving just a quadratic term, then the test RSS of the linear and cubic models may be quite close. However, if the true relationship includes stronger non-linear components, especially a third-degree term or higher, the cubic model is more likely to capture that structure and achieve a lower test RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf218de",
   "metadata": {},
   "source": [
    "**5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form**\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = x_i\\hat{\\beta}\n",
    "$$ \n",
    "**where** \n",
    "$$\n",
    "\\hat{\\beta} = \\sum_{i'=1}^{n} x_{i'} y_{i'} / \\sum_{i''=1}^{n} x_{i''}^2\n",
    "$$\n",
    "**Show that we can write**\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{i'=1}^{n} a_{i'} y_{i'}\n",
    "$$\n",
    "**What is $a_{i'}$ ?**\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = x_i \\frac{\\sum_{i'=1}^{n} x_{i'} y_{i'}}{\\sum_{i''=1}^{n} x_{i''}^2}\n",
    "$$\n",
    "Let $b=\\sum_{i''=1}^{n} x_{i''}^2$\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{i'=1}^{n} \\frac{x_{i'} * x_i}{b} y_{i'}\n",
    "= \\sum_{i'=1}^{n} a_{i'} y_{i'}\n",
    "$$\n",
    "\n",
    "Where \n",
    "$$\n",
    "a_{i'}= \\frac{x_{i'} * x_i}{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fa72a",
   "metadata": {},
   "source": [
    "**6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\\bar{x}, \\bar{y})$.**\n",
    "\n",
    "We already know that: $\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 x$, So, for $x=\\bar{x}$ We have:\n",
    "$$\n",
    "\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\n",
    "$$\n",
    "Since $\\hat{\\beta}_0=\\bar{y} - \\hat{\\beta}_1 \\bar{x}$\n",
    "$$\n",
    "\\hat{y}=\\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x} = \\bar{y} \\Longrightarrow \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\n",
    "$$\n",
    "So, we can conclude that in the case of simple linear regression, the least squares line always passes through $(\\bar{x}, \\bar{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d2533",
   "metadata": {},
   "source": [
    "**7. It is claimed in the text that in the case of simple linear regression of $Y$ onto $X$, the $R^2$ statistic (3.17) is equal to the square of the correlation between $X$ and $Y$ (3.18). Prove that this is the case. For simplicity, you may assume that $\\bar{x} = \\bar{y} = 0$.**\n",
    "\n",
    "We want to prove that:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Since we're assuming $\\bar{x} = \\bar{y} = 0$, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n y_i^2}\n",
    "$$\n",
    "\n",
    "**Demonstration:**\n",
    "\n",
    "$$\n",
    "1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n y_i^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 - \\frac{\\sum_{i=1}^n y_i^2 - 2 \\sum_{i=1}^n \\hat{y}_i y_i + \\sum_{i=1}^n \\hat{y}_i^2}{\\sum_{i=1}^n y_i^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{2 \\sum_{i=1}^n \\hat{y}_i y_i - \\sum_{i=1}^n \\hat{y}_i^2}{\\sum_{i=1}^n y_i^2}\n",
    "$$\n",
    "\n",
    "We know that $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$, and with $\\bar{x} = \\bar{y} = 0$, it follows that:\n",
    "\n",
    "* $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = 0$\n",
    "* $\\hat{y}_i = \\hat{\\beta}_1 x_i$\n",
    "* $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}$\n",
    "\n",
    "Substitute into the previous expression:\n",
    "\n",
    "$$\n",
    "\\frac{2 \\sum_{i=1}^n \\hat{y}_i y_i - \\sum_{i=1}^n \\hat{y}_i^2}{\\sum_{i=1}^n y_i^2}\n",
    "= \\frac{2 \\hat{\\beta}_1 \\sum_{i=1}^n x_i y_i - \\hat{\\beta}_1^2 \\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n y_i^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{2 \\left( \\frac{\\sum x_i y_i}{\\sum x_i^2} \\right) \\sum x_i y_i - \\left( \\frac{\\sum x_i y_i}{\\sum x_i^2} \\right)^2 \\sum x_i^2}{\\sum y_i^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{2 \\frac{(\\sum x_i y_i)^2}{\\sum x_i^2} - \\frac{(\\sum x_i y_i)^2}{\\sum x_i^2}}{\\sum y_i^2}\n",
    "= \\frac{(\\sum x_i y_i)^2}{\\sum x_i^2 \\sum y_i^2}\n",
    "$$\n",
    "$$\n",
    "= \\left( \\frac{\\sum x_i y_i}{\\sqrt{\\sum x_i^2} \\sqrt{\\sum y_i^2}} \\right)^2\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum y_i^2} = \\left( \\frac{\\sum x_i y_i}{\\sqrt{\\sum x_i^2} \\sqrt{\\sum y_i^2}} \\right)^2\n",
    "$$\n",
    "\n",
    "So we conclude:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^n x_i y_i}{\\sqrt{\\sum_{i=1}^n x_i^2} \\sqrt{\\sum_{i=1}^n y_i^2}} = \\sqrt{R^2} \\Rightarrow R^2 = \\left( \\text{Cor}(X, Y) \\right)^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
