{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4773c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ISLP import load_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NYSE = load_data('NYSE')\n",
    "cols = ['DJ_return', 'log_volume', 'log_volatility']\n",
    "\n",
    "X = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(NYSE[cols]),\n",
    "    columns=cols,\n",
    "    index=NYSE.index\n",
    ")\n",
    "\n",
    "lags = 5\n",
    "X_lagged = pd.concat([X.shift(i) for i in range(1, lags+1)], axis=1)\n",
    "X_lagged.columns = [f\"{col}_lag{i}\" for i in range(1, lags+1) for col in cols]\n",
    "\n",
    "X_lagged = X_lagged.dropna()\n",
    "Y = X.loc[X_lagged.index, 'log_volume']\n",
    "X_flat = X_lagged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35838e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(range(len(X_flat)), test_size=0.2, shuffle=False)\n",
    "X_train, X_test = X_flat.iloc[train_idx], X_flat.iloc[test_idx]\n",
    "Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e35383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R^2 (flattened sequences): 0.3912\n"
     ]
    }
   ],
   "source": [
    "lr_flat = LinearRegression()\n",
    "lr_flat.fit(X_train, Y_train)\n",
    "\n",
    "r2_flat = lr_flat.score(X_test, Y_test)\n",
    "print(f\"Test R^2 (flattened sequences): {r2_flat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeddf617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R^2 (standard AR): 0.3912\n"
     ]
    }
   ],
   "source": [
    "lr_standard = LinearRegression()\n",
    "lr_standard.fit(X_train, Y_train)\n",
    "r2_standard = lr_standard.score(X_test, Y_test)\n",
    "print(f\"Test R^2 (standard AR): {r2_standard:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbed0b8",
   "metadata": {},
   "source": [
    "Both the flattened sequence approach and the standard linear AR model yield the same **test RÂ² of 0.391**, indicating that they explain about 39% of the variance in the target variable **log_volume**. This equality occurs because, for a linear model, flattening the lagged sequences does not change the information content: each approach uses the same lagged predictors. The advantage of flattening sequences lies primarily in its compatibility with **RNNs and other sequence models**, where a 3D input of **(samples, timesteps, features)** is required. In contrast, the standard AR approach is simpler, more interpretable, and sufficient for linear regression on lagged data. Thus, while both methods perform identically in this linear setting, flattening becomes more relevant when extending to nonlinear or deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb9647",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
